{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.distributions as D\n",
    "from tqdm import tqdm, trange\n",
    "from src.svgd import SVGD\n",
    "from src.gsvgd import FullGSVGDBatch\n",
    "from src.kernel import RBF, BatchRBF\n",
    "from src.utils import plot_particles\n",
    "from src.Tmy_svgd import tmySVGD\n",
    "from src.manifold import Grassmann\n",
    "from src.s_svgd import SlicedSVGD\n",
    "from src.mysvgd import mySVGD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import pickle\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import torch.autograd as autograd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_gauss_experiment(mixture_dist, means):\n",
    "    '''Mixture of Multivariate gaussian with cov matrices being the identity.\n",
    "    Args:\n",
    "        probs: Tensor of shape (nmix,) for the mixture_distribution.\n",
    "        means: Tensor of shape (nmix, d), where nmix is the number of components \n",
    "            and d is the dimension of each component.\n",
    "    '''\n",
    "    nmix = means.shape[0]\n",
    "    comp = D.Independent(D.Normal(means.to(device), torch.ones((nmix, means.shape[1]), device=device)), 1)\n",
    "    distribution = D.mixture_same_family.MixtureSameFamily(mixture_dist, comp) \n",
    "    return distribution\n",
    "\n",
    "\n",
    "def points_on_circle(theta, rad):\n",
    "    '''Generate d-dim points whose first two dimensions lies on a circle of \n",
    "    radius rad, with position being specified by the angle from the positive \n",
    "    x-axis theta.\n",
    "    '''\n",
    "    return torch.Tensor([[rad * np.cos(theta + 0.25*np.pi), rad * np.sin(theta + 0.25*np.pi)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Running xshaped experiment.')\n",
    "parser.add_argument('--dim', type=int,default=20, help='dimension')\n",
    "parser.add_argument('--effdim', type=int, default=-1, help='dimension')\n",
    "parser.add_argument('--lr', type=float,default=0.01, help='learning rate')\n",
    "parser.add_argument('--lr_g', type=float, default=0.1, help='learning rate for g')\n",
    "parser.add_argument('--delta', type=float,default=0.01, help='stepsize for projections')\n",
    "parser.add_argument('--T', type=float, default=1e-4, help='noise multiplier for projections')\n",
    "parser.add_argument('--nparticles', type=int,default=200, help='no. of particles')\n",
    "parser.add_argument('--epochs', type=int, default=20000,help='no. of epochs')\n",
    "parser.add_argument('--nmix', type=int, default=4, help='no. of modes')\n",
    "parser.add_argument('--metric', type=str, default=\"energy\", help='distance metric')\n",
    "parser.add_argument('--noise', type=str, default=\"True\", help='whether to add noise')\n",
    "parser.add_argument('--kernel', type=str, default=\"rbf\", help='kernel')\n",
    "parser.add_argument('--gpu', type=int, default=2, help='gpu')\n",
    "parser.add_argument('--seed', type=int, default=0, help='random seed') \n",
    "parser.add_argument('--suffix', type=str, default=\"\", help='suffix for res folder')\n",
    "parser.add_argument('--m', type=int, help='no. of projections')\n",
    "parser.add_argument('--save_every', type=int, default=200, help='step intervals to save particles')\n",
    "parser.add_argument('--method', type=str, default=\"all\", help='which method to use')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args([])\n",
    "device = torch.device(f'cuda:{args.gpu}' if args.gpu != -1 else 'cpu')\n",
    "dim = args.dim\n",
    "lr = args.lr\n",
    "delta = args.delta\n",
    "T = args.T\n",
    "nparticles = args.nparticles\n",
    "epochs = args.epochs\n",
    "seed = args.seed\n",
    "eff_dims = [args.effdim] if args.effdim > 0 else [1, 2, 5]\n",
    "nmix = args.nmix\n",
    "add_noise = True if args.noise == \"True\" else False\n",
    "radius = 5\n",
    "save_every = args.save_every\n",
    "print(f\"Running for dim: {dim}, lr: {lr}, nparticles: {nparticles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(f'cuda:{args.gpu}' if args.gpu != -1 else 'cpu')\n",
    "\n",
    "metric = args.metric\n",
    "\n",
    "results_folder = f\"./res/multimodal{args.suffix}/{args.kernel}_epoch{epochs}_lr{lr}_delta{delta}_n{nparticles}_dim{dim}\"\n",
    "results_folder = f\"{results_folder}/seed{seed}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)\n",
    "\n",
    "if args.kernel == \"rbf\":\n",
    "    Kernel = RBF\n",
    "    BatchKernel = BatchRBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Device: {device}\")\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "## target density\n",
    "mix_means = torch.cat(\n",
    "    [points_on_circle(i * 2*np.pi / nmix, rad=radius) for i in range(nmix)]).to(device)\n",
    "mix_means = torch.cat((mix_means, torch.zeros((mix_means.shape[0], dim - 2), device=device)), dim=1)\n",
    "\n",
    "distribution = mix_gauss_experiment(\n",
    "    mixture_dist=D.Categorical(torch.ones(mix_means.shape[0], device=device)),\n",
    "    means=mix_means\n",
    ")\n",
    "\n",
    "# sample from target (for computing metric)\n",
    "x_target = distribution.sample((nparticles, ))\n",
    "# sample from variational density\n",
    "x_init =  torch.randn(nparticles, *distribution.event_shape).to(device)\n",
    "\n",
    "## SVGD\n",
    "if args.method in [\"SVGD\", \"all\"]:\n",
    "    print(\"Running SVGD\")\n",
    "    # sample from variational density\n",
    "    x = x_init.clone().to(device)\n",
    "    kernel = Kernel(method=\"med_heuristic\")\n",
    "    svgd = SVGD(distribution, kernel, optim.Adam([x], lr=lr), device=device)\n",
    "    start = time.time()\n",
    "    svgd.fit(x, epochs, verbose=True, save_every=save_every)\n",
    "    elapsed_time_svgd = time.time() - start\n",
    "\n",
    "    # plot particles\n",
    "    fig_svgd = plot_particles(\n",
    "        x_init.detach(), \n",
    "        x.detach(), \n",
    "        distribution, \n",
    "        d=9.0, \n",
    "        step=0.1, \n",
    "        concat=mix_means[0, 2:],\n",
    "        savedir=results_folder + f\"/svgd.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running mySVGD\")\n",
    "\n",
    "def score(X):\n",
    "        X_cp = X.clone().detach().requires_grad_()\n",
    "        log_prob = distribution.log_prob(X_cp)\n",
    "        score_func = autograd.grad(log_prob.sum(), X_cp)[0]\n",
    "        return score_func\n",
    "\n",
    "\n",
    "# sample from variational density\n",
    "res = []\n",
    "rres = []\n",
    "steps = []\n",
    "lr = 0.01\n",
    "x0 = x_init\n",
    "vector1  = torch.randn(nparticles, dim).to(device)\n",
    "\n",
    "for i in trange(200):\n",
    "    \n",
    "    theta, vector = mySVGD().update(x0, score,  k = 2, n_iter = 50,  debug = False, lr= lr, vector=vector1, device = device)\n",
    "    #mean = np.mean(theta, axis=0)  + np.random.random(1)\n",
    "    #var_theta = np.cov(theta.T) + np.random.random(1)\n",
    "    #x0 = np.random.multivariate_normal(mean, var_theta,num)\n",
    "    x0 = theta\n",
    "    vector1 = vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_svgd = plot_particles(\n",
    "        x_init.detach(), \n",
    "        theta.detach(), \n",
    "        distribution, \n",
    "        d=9.0, \n",
    "        step=0.1, \n",
    "        concat=mix_means[0, 2:]\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running tmySVGD\")\n",
    "\n",
    "def score(X):\n",
    "        X_cp = X.clone().detach().requires_grad_()\n",
    "        log_prob = distribution.log_prob(X_cp)\n",
    "        score_func = autograd.grad(log_prob.sum(), X_cp)[0]\n",
    "        return score_func\n",
    "\n",
    "\n",
    "# sample from variational density\n",
    "res = []\n",
    "rres = []\n",
    "steps = []\n",
    "lr = 0.1\n",
    "x0 = x_init\n",
    "vector1  = torch.randn(nparticles, dim).to(device)\n",
    "\n",
    "for i in trange(100):\n",
    "    \n",
    "    theta, vector = tmySVGD(kernel).update(x0, score,  k = 2, n_iter = 50,  debug = False, lr= lr, vector=vector1)\n",
    "    #mean = np.mean(theta, axis=0)  + np.random.random(1)\n",
    "    #var_theta = np.cov(theta.T) + np.random.random(1)\n",
    "    #x0 = np.random.multivariate_normal(mean, var_theta,num)\n",
    "    x0 = theta\n",
    "    vector1 = vector\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_svgd = plot_particles(\n",
    "        x_init.detach(), \n",
    "        theta.detach(), \n",
    "        distribution, \n",
    "        d=9.0, \n",
    "        step=0.1, \n",
    "        concat=mix_means[0, 2:]\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GSVGD\n",
    "if args.method in [\"GSVGD\", \"all\"]:\n",
    "    res_gsvgd = [0] * len(eff_dims)\n",
    "    def run_gsvgd(eff_dims):\n",
    "        for i, eff_dim in enumerate(eff_dims):\n",
    "            print(f\"Running GSVGD with eff dim = {eff_dim}\")\n",
    "\n",
    "            m = min(20, dim // eff_dim) if args.m is None else args.m\n",
    "            print(\"number of projections:\", m)\n",
    "\n",
    "            # sample from variational density\n",
    "            x_init_gsvgd = x_init.clone()\n",
    "            x_gsvgd = x_init_gsvgd.clone()\n",
    "\n",
    "            kernel_gsvgd = BatchKernel(method=\"med_heuristic\")\n",
    "            optimizer = optim.Adam([x_gsvgd], lr=lr)\n",
    "            manifold = Grassmann(dim, eff_dim)\n",
    "            U = torch.eye(dim, device=device).requires_grad_(True)\n",
    "            U = U[:, :(m*eff_dim)]\n",
    "\n",
    "            gsvgd = FullGSVGDBatch(\n",
    "                target=distribution,\n",
    "                kernel=kernel_gsvgd,\n",
    "                manifold=manifold,\n",
    "                optimizer=optimizer,\n",
    "                delta=delta,\n",
    "                T=T,\n",
    "                device=device,\n",
    "                noise=add_noise\n",
    "            )\n",
    "            start = time.time()\n",
    "            U, metric_gsvgd = gsvgd.fit(x_gsvgd, U, m, epochs, \n",
    "                verbose=True, save_every=save_every, threshold=0.0001*m)\n",
    "            elapsed_time = time.time() - start\n",
    "\n",
    "            # plot particles\n",
    "            fig_gsvgd = plot_particles(\n",
    "                x_init_gsvgd.detach(), \n",
    "                x_gsvgd.detach(), \n",
    "                distribution, \n",
    "                d=9.0, \n",
    "                step=0.1, \n",
    "                concat=mix_means[0, 2:].to(device),\n",
    "                savedir=results_folder + f\"/fullgsvgd_effdim{eff_dim}_lr{lr}_delta{delta}_m{m}_T{T}.png\"\n",
    "            )\n",
    "\n",
    "            # store results\n",
    "            res_gsvgd[i] = {\n",
    "                \"init\":x_init_gsvgd, \"final\":x_gsvgd, \"metric\":metric_gsvgd, \n",
    "                \"fig\":fig_gsvgd, \"particles\":gsvgd.particles, \"pam\":gsvgd.pam, \"res\": gsvgd,\n",
    "                \"elapsed_time\": elapsed_time}\n",
    "        return res_gsvgd\n",
    "\n",
    "    res_gsvgd = run_gsvgd(eff_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## S-SVGD\n",
    "if args.method in [\"S-SVGD\", \"all\"]:\n",
    "    # sample from variational density\n",
    "    print(\"Running S-SVGD\")\n",
    "    x_init_s_svgd = x_init.clone()\n",
    "    x_s_svgd = x_init_s_svgd.clone().requires_grad_()\n",
    "    s_svgd = SlicedSVGD(distribution, device=device)\n",
    "\n",
    "    start = time.time()\n",
    "    x_s_svgd, metric_s_svgd = s_svgd.fit(\n",
    "        samples=x_s_svgd, \n",
    "        n_epoch=epochs, \n",
    "        lr=args.lr_g,\n",
    "        eps=lr,\n",
    "        save_every=save_every\n",
    "    )\n",
    "    elapsed_time_s_svgd = time.time() - start\n",
    "\n",
    "    # plot particles\n",
    "    fig_s_svgd = plot_particles(\n",
    "        x_init_s_svgd.detach(), \n",
    "        x_s_svgd.detach(), \n",
    "        distribution, \n",
    "        d=9.0, \n",
    "        step=0.1, \n",
    "        concat=mix_means[0, 2:],\n",
    "        savedir=results_folder + f\"/ssvgd_lr{lr}_lrg{args.lr_g}.png\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('AG')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28e57e6ed411ebe457d9d5e52f3313a26a0295904f732baf128c9b6661c268ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
